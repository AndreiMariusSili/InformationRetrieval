{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also run a correlation analysis to evaluate agreement between degrees of preference for online and offline evaluation measures. All measures are standardized with a mean of 0 and a standard deviation of 1 of more robust comparisons. Although correlation coefficients do not generally achieve high values, the plots below give a better picture. It can be seen that average precision does not correlate in any significant way with Simple Dependent Click Model. Given this discovery, and the previous discussion about the disconenct between binary relevance indicators and nuanced user behaviour, we choose to again exclude average precision for further analysis.\n",
    "\n",
    "However, a particular trend can be noticed for Discounted Cumulative Gain and Expected Reciprocal Rank. As the offline metric increases, so does the agreement with the online measure. So, when an offline measure is not particularly sure of whether a retrieval algorithm is better than another, (simulated) online behaviour will tend to fluctuate, which is a reasonable observation given the variance in user preferences. However, a strong offlie judgement will tipycally result in convergent online behaviour for both ERR and nDCG.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
