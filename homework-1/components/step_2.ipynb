{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as cl\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_precision(ranking, denominator_R=10, denominator_HR=10):\n",
    "    \"\"\"Compute average of precisions at relevant and highly-relevant documents. \n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        denominator_R: Total relevant documents in collection\n",
    "        denominator_HR: Total highly-relevant documents in collection\n",
    "    Returns:\n",
    "        Tuple with average of precisions of relevant and highly-relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    R_precision_sum = 0\n",
    "    R_docs = 0\n",
    "    \n",
    "    HR_precisions_sum = 0\n",
    "    HR_docs = 0\n",
    "    \n",
    "    for i in range(len(ranking)):\n",
    "        if ranking[i] == 1:\n",
    "            R_docs += 1\n",
    "            R_precision_sum += (R_docs / (i + 1))\n",
    "        elif ranking[i] == 2:\n",
    "            HR_docs += 1\n",
    "            HR_precisions_sum += (HR_docs / (i + 1))\n",
    "    \n",
    "    return (R_precision_sum / denominator_R, HR_precisions_sum / denominator_HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_DCG(ranking, k=5):\n",
    "    \"\"\"Compute Discounted Cumulative Gain at rank k. \n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        k: Position on which DCG is computed; by default, 5\n",
    "    Returns:\n",
    "        DCG value at rank k\n",
    "    \"\"\"\n",
    "    \n",
    "    DCG = 0\n",
    "    for i in range(k):\n",
    "        DCG += (2 ** ranking[i] - 1) / (math.log2(i + 2))\n",
    "    \n",
    "    return DCG\n",
    "\n",
    "def compute_nDCG(ranking, k=5, maxDCG=1):\n",
    "    \"\"\"Compute normalized Discounted Cumulative Gain at rank k. \n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        k: Position on which DCG is computed; by default, 5\n",
    "        maxDCG: DCG for best possible ranking for normalisation\n",
    "    Returns:\n",
    "        Normalized DCG value at rank k\n",
    "    \"\"\"\n",
    "    \n",
    "    return compute_DCG(ranking, k) / maxDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_theta(rel, max_rel=2):\n",
    "    \"\"\"Compute probability of satisfaction.\n",
    "    \n",
    "    Args:\n",
    "        rel: Relevance score of certain document\n",
    "        max_rel: Maximum possible relevance\n",
    "    Returns:\n",
    "        Probability of satisfaction\n",
    "    \"\"\"\n",
    "    return (2 ** rel - 1) / (2 ** max_rel)\n",
    "\n",
    "def compute_ERR(ranking, max_rel=2):\n",
    "    \"\"\"Compute Expected Reciprocal Rank.\n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        max_rel: Maximum possible relevance of documents\n",
    "    Returns:\n",
    "        Expected Reciprocal Rank measure\n",
    "    \"\"\"\n",
    "    ERR = 0\n",
    "    \n",
    "    for i in range(len(ranking)):\n",
    "        proba = 1\n",
    "        theta_i = compute_theta(ranking[i], max_rel)\n",
    "        for j in range(i):\n",
    "            proba *= (1 - compute_theta(ranking[j], max_rel)) * theta_i\n",
    "        \n",
    "        ERR += proba / (i + 1)\n",
    "    \n",
    "    return ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(rank_pairs):\n",
    "    \"\"\"Retrieve the 3 evaluation values for pairs of relevance ranking.\n",
    "    \n",
    "    Args:\n",
    "        rank_pairs: Relevance ranking pairs for P and E\n",
    "    Returns:\n",
    "        Dictionary of AP, nDCG, ERR scores\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    maxDCG = compute_DCG((2, 2, 2, 2, 2))\n",
    "    \n",
    "    for pair in rank_pairs:\n",
    "        pair_scores = {}\n",
    "        \n",
    "        pair_scores['AP_P_R'], pair_scores['AP_P_HR'] = compute_avg_precision(pair.P)\n",
    "        pair_scores['AP_E_R'], pair_scores['AP_E_HR'] = compute_avg_precision(pair.E)\n",
    "        \n",
    "        pair_scores['nDCG_P'] = compute_nDCG(pair.P, maxDCG=maxDCG)\n",
    "        pair_scores['nDCG_E'] = compute_nDCG(pair.E, maxDCG=maxDCG)\n",
    "        \n",
    "        pair_scores['ERR_P'] = compute_ERR(pair.P)\n",
    "        pair_scores['ERR_E'] = compute_ERR(pair.E)\n",
    "        \n",
    "        scores[pair] = pair_scores\n",
    "        \n",
    "    return scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
