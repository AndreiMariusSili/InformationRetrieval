{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Part\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hypothesis Testing â€“ The problem of multiple comparisons [5 points]\n",
    "\n",
    "The problem of multiple comparisons can be viewed in terms of a Bernoulli experiment over multipe hypothesis tests, in which the Type I Error probability $\\alpha$ of each hypothesis test is independent of the the previous tests. We treat the probability of Type I Error as the probability of a success in the Bernoulli experiment. Thus, for any one experiment, we have a probability distribtion governed by the following parameters as follows:\n",
    "\n",
    "* chance of making a Type I Error: $\\alpha$\n",
    "* chance of not making a Type I Error: $1 - \\alpha$\n",
    "\n",
    "Given this setup, a collection of $m$ hypothesis tests generates a binomial distribution for the chance of observing $k$ Type I Errors. The distribution is governed by the following parameters:\n",
    "\n",
    "* number of trials = $m$\n",
    "* number of successes = $k$\n",
    "* probability of Type I Error = $alpha$\n",
    "\n",
    "The probability mass functions for the binomial distribution is given by:\n",
    "\n",
    "$$Pr(k\\ |\\ n, p) = C_n^k p^k (1-p)^{n-k} = \\frac{n!}{k!(n-k)!} p^k (1-p)^{n-k}$$\n",
    "\n",
    "Given this reasoning, we can answer the questions as follows:\n",
    "\n",
    "$$P(m^{th}\\ experiment\\ gives\\ significant\\ result\\ |\\ m experiments\\ lacking\\ power\\ to\\ reject\\ H_0) = Pr(Type\\ I\\ Error) = \\alpha$$\n",
    "\n",
    "$$P(at\\ least\\ one\\ significant\\ result\\ |\\ m\\ experiments\\ lacking\\ power\\ to\\ reject\\ H_0) = Pr(k=1\\ |\\ n=m, p=\\alpha) = \\frac{m!}{(m-1)!} \\alpha(1-\\alpha)^{m-1}$$\n",
    "\n",
    "\n",
    "\n",
    "### 2. Bias and unfairness in Interleaving experiments [10 points]\n",
    "A scenario in which a team-draft interleaving algorithm is insensitive between 2 ranked lists of length 4 is detailed in Hoffman et al. (2011). A similar argument is made here for length 3. The scenario is showed in the figure below.\n",
    "\n",
    "![caption](files/team-draft-interleaving-insensitivity.png)\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "HOFMANN, K., WHITESON, S., AND DE RIJKE, M. 2011. A probabilistic method for inferring preferences from\n",
    "clicks. In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Part\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as cl\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Rankings of Relevance for  E  and  P (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rank_col(rank_len:int=5, rel_set:tuple=(0,1,2))->list:\n",
    "    \"\"\" Generate collection of all possible rankings.\n",
    "    \n",
    "    Args:\n",
    "        rank_len: Length of each ranking.\n",
    "        rel_set: Set of relevance scores.\n",
    "    Returns:\n",
    "        The cartesian product of of `rel_set` repeated `rank_len` times.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [x for x in itertools.product(rel_set, repeat=rank_len)]\n",
    "\n",
    "def generate_rank_pairs(rank_col:list)->list:\n",
    "    \"\"\" Generate all possible pairs of rankings.\n",
    "    \n",
    "    Args:\n",
    "        rank_col: a collection of rankings\n",
    "    Returns:\n",
    "        The cartesian product of a ranking repeated twice.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [Pair(*x) for x in itertools.product(rank_col, repeat=2)]\n",
    "\n",
    "Pair = cl.namedtuple('Pair', ['P', 'E'])\n",
    "RANKING_LENGTH = 5\n",
    "REL_SET = (0,1,2)\n",
    "\n",
    "rank_pairs = generate_rank_pairs(generate_rank_col(RANKING_LENGTH, REL_SET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 :  Implement Evaluation Measures  (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Average Precision\n",
    "def compute_avg_precision(ranking, denominator_R=10, denominator_HR=10):\n",
    "    \"\"\"Compute average of precisions at relevant and highly-relevant documents. \n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        denominator_R: Total relevant documents in collection\n",
    "        denominator_HR: Total highly-relevant documents in collection\n",
    "    Returns:\n",
    "        Tuple with average of precisions of relevant and highly-relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    R_precision_sum = 0\n",
    "    R_docs = 0\n",
    "    \n",
    "    HR_precisions_sum = 0\n",
    "    HR_docs = 0\n",
    "    \n",
    "    for i in range(len(ranking)):\n",
    "        if ranking[i] == 1:\n",
    "            R_docs += 1\n",
    "            R_precision_sum += (R_docs / (i + 1))\n",
    "        elif ranking[i] == 2:\n",
    "            HR_docs += 1\n",
    "            HR_precisions_sum += (HR_docs / (i + 1))\n",
    "    \n",
    "    return (R_precision_sum / denominator_R, HR_precisions_sum / denominator_HR)\n",
    "\n",
    "### nDCG@k\n",
    "def compute_DCG(ranking, k=5):\n",
    "    \"\"\"Compute Discounted Cumulative Gain at rank k. \n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        k: Position on which DCG is computed; by default, 5\n",
    "    Returns:\n",
    "        DCG value at rank k\n",
    "    \"\"\"\n",
    "    \n",
    "    DCG = 0\n",
    "    for i in range(k):\n",
    "        DCG += (2 ** ranking[i] - 1) / (math.log2(i + 2))\n",
    "    \n",
    "    return DCG\n",
    "\n",
    "def compute_nDCG(ranking, k=5, maxDCG=1):\n",
    "    \"\"\"Compute normalized Discounted Cumulative Gain at rank k. \n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        k: Position on which DCG is computed; by default, 5\n",
    "        maxDCG: DCG for best possible ranking for normalisation\n",
    "    Returns:\n",
    "        Normalized DCG value at rank k\n",
    "    \"\"\"\n",
    "    \n",
    "    return compute_DCG(ranking, k) / maxDCG\n",
    "\n",
    "### ERR\n",
    "def compute_theta(rel, max_rel=2):\n",
    "    \"\"\"Compute probability of satisfaction.\n",
    "    \n",
    "    Args:\n",
    "        rel: Relevance score of certain document\n",
    "        max_rel: Maximum possible relevance\n",
    "    Returns:\n",
    "        Probability of satisfaction\n",
    "    \"\"\"\n",
    "    return (2 ** rel - 1) / (2 ** max_rel)\n",
    "\n",
    "def compute_ERR(ranking, max_rel=2):\n",
    "    \"\"\"Compute Expected Reciprocal Rank.\n",
    "    \n",
    "    Args:\n",
    "        ranking: Relevance ranking\n",
    "        max_rel: Maximum possible relevance of documents\n",
    "    Returns:\n",
    "        Expected Reciprocal Rank measure\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    \n",
    "    for i in range(len(ranking)):\n",
    "        R = compute_theta(ranking[i], max_rel)\n",
    "        ERR = ERR + p * R / (i + 1)\n",
    "        p = p * (1 - R)\n",
    "    \n",
    "    return ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_scores = {}\n",
    "maxDCG = compute_DCG((2, 2, 2, 2, 2))\n",
    "\n",
    "for pair in rank_pairs:\n",
    "    pair_scores = {}\n",
    "\n",
    "    pair_scores['AP_P_R'], pair_scores['AP_P_HR'] = compute_avg_precision(pair.P)\n",
    "    pair_scores['AP_E_R'], pair_scores['AP_E_HR'] = compute_avg_precision(pair.E)\n",
    "\n",
    "    pair_scores['nDCG_P'] = compute_nDCG(pair.P, maxDCG=maxDCG)\n",
    "    pair_scores['nDCG_E'] = compute_nDCG(pair.E, maxDCG=maxDCG)\n",
    "\n",
    "    pair_scores['ERR_P'] = compute_ERR(pair.P)\n",
    "    pair_scores['ERR_E'] = compute_ERR(pair.E)\n",
    "\n",
    "    initial_scores[pair] = pair_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 :  Calculate the  ð›¥measure (0 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_scores = {}\n",
    "\n",
    "for key in initial_scores:\n",
    "    deltas = {}\n",
    "    score = initial_scores[key]\n",
    "\n",
    "    if score['AP_E_R'] > score['AP_P_R']:\n",
    "        deltas['AP_R'] = score['AP_E_R'] - score['AP_P_R']\n",
    "\n",
    "    if score['AP_E_HR'] > score['AP_P_HR']:\n",
    "        deltas['AP_HR'] = score['AP_E_HR'] - score['AP_P_HR']\n",
    "\n",
    "    if score['nDCG_E'] > score['nDCG_P']:\n",
    "        deltas['nDCG'] = score['nDCG_E'] - score['nDCG_P']\n",
    "\n",
    "    if score['ERR_E'] > score['ERR_P']:\n",
    "        deltas['ERR'] = score['ERR_E'] - score['ERR_P']\n",
    "\n",
    "    if deltas:\n",
    "        pair_scores[key] = deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 :  Implement Interleaving ( 15 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Team_Draft_Interleaving():\n",
    "    def __init__(self):\n",
    "        # Variables hold the interleaved ranking, and the teams.\n",
    "        # The items are identified by the index in the interleaved ranking.\n",
    "        self.rank_i, self.team_e, self.team_p = list(),list(),list()\n",
    "\n",
    "        # Variable holds the scores of each team.\n",
    "        self.clicks = {'stream': [], 'scores': {'E': 0, 'P': 0}}\n",
    "    \n",
    "    def interleave_pairs(self, pair: Pair) -> (list, list, list):\n",
    "        \"\"\"Interleaves a pair of rankings using team draft method.\n",
    "\n",
    "        Args:\n",
    "            pair: The pair of experimental and production rankings.\n",
    "        Returns:\n",
    "            Interleaved ranking and teams  \n",
    "        \"\"\"\n",
    "        rank_e, rank_p = pair.E, pair.P    \n",
    "        count_e, count_p = 0, 0\n",
    "\n",
    "        # Algorithm is implemented according to slides, with some simplifications. Because all documents are\n",
    "        # assumed unique, the checks become simpler. Every time an item from one rank is added to the interleaved result,\n",
    "        # the item is added to the team and counter for that list is incremented. When both counters reach the length\n",
    "        # of the their respective rankings, both lists have been exhausted.\n",
    "        while count_e < len(rank_e) and count_p < len(rank_p):\n",
    "            pick_team_e = (len(self.team_e) < len(self.team_p)) or (len(self.team_e) == len(self.team_p) and random.choice((True, False)))\n",
    "            if pick_team_e:\n",
    "                self.rank_i.append(rank_e[count_e])\n",
    "                self.team_e.append(len(self.rank_i)-1)\n",
    "                count_e += 1\n",
    "            else:\n",
    "                self.rank_i.append(rank_p[count_p])\n",
    "                self.team_p.append(len(self.rank_i)-1)\n",
    "                count_p += 1\n",
    "        return self.rank_i[:len(pair.E)], self.team_e, self.team_p\n",
    "    \n",
    "    def assign_click(self, index: int) -> str:\n",
    "        \"\"\"Assign simulated click to the owner of the item.\n",
    "\n",
    "        Args:\n",
    "            index: The index of the item clicked.\n",
    "        Return:\n",
    "            A string 'E' or 'P' representing the owner of the item.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.clicks['stream'].append(index)\n",
    "        if index in self.team_e:\n",
    "            self.clicks['scores']['E'] += 1\n",
    "            return 'E'\n",
    "        elif index in self.team_p:\n",
    "            self.clicks['scores']['P'] += 1\n",
    "            return 'P'\n",
    "        else:\n",
    "            raise IndexError('Index {} not in either teams'.format(index))\n",
    "            \n",
    "    def reset_clicks_history(self):\n",
    "        \"\"\"Reset clicks variable to clear history\"\"\"\n",
    "        self.clicks = {'stream': [], 'scores': {'E': 0, 'P': 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking_pair = Pair((2,1,0,1,1), (1,0,1,2,1)) # actual main would probably loop over the ranking pairs generate at step 1\n",
    "\n",
    "# team_draft_algo = Team_Draft_Interleaving()\n",
    "# team_draft_algo.team_draft_interleave(ranking_pair)\n",
    "\n",
    "# for i in range(5):\n",
    "#     team_draft_algo.assign_click(random.choice(range(9)))\n",
    "    \n",
    "# team_draft_algo.clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 :  Implement User Clicks Simulation ( 15 points )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Click model (RCM)\n",
    "\n",
    "### $\\rho = \\frac{\\sum_{s\\in S}\\sum_{u\\in s}c_{u}^{(s)}}{\\sum_{s\\in S}\\vert S \\vert}$\n",
    "\n",
    "In other words, $\\rho = \\frac{ \\text{number of clicks}}{\\text{number of documents shown}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./resources/YandexRelPredChallenge.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCM:\n",
    "    def __init__(self, log_filename):\n",
    "        log_file = open(log_filename)\n",
    "        self.probability = self.get_parameter(log_file)\n",
    "        \n",
    "    def _is_querry(self, line):\n",
    "        return line.split()[2].lower() == 'q'\n",
    "\n",
    "    def _get_url_list(self, line):\n",
    "        assert line.split()[2].lower() == 'q'\n",
    "        return line[5:]\n",
    "    \n",
    "    def get_parameter(self, training_data):\n",
    "        \"\"\"\n",
    "        (a) A method that learns the parameters of the model given a set of training data.\n",
    "        \"\"\"\n",
    "        documents_shown = 0\n",
    "        clicks = 0\n",
    "        for line in training_data:\n",
    "            if self._is_querry(line): # is querry\n",
    "                url_list = self._get_url_list(line)\n",
    "                number_of_urls = len(url_list)\n",
    "                documents_shown += number_of_urls\n",
    "            else:# is click\n",
    "                clicks += 1\n",
    "        return clicks / documents_shown\n",
    "    \n",
    "    def click_probabilities(self, urls):\n",
    "        \"\"\"\n",
    "        (b) A method that predicts the click probability given a ranked list of relevance labels.\n",
    "            For RCM, all links have the same probability.\n",
    "        \"\"\"\n",
    "        return [self.probability for i in range(0,len(urls))]\n",
    "    \n",
    "    def clicks(self, click_probabilities):\n",
    "        \"\"\"\n",
    "        (c) A method that decides - stochastically - whether a document is clicked based on their probabilities.\n",
    "        \"\"\"\n",
    "        return [np.random.binomial(1, prob) for prob in click_probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcm_model = RCM(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Simple Dependent Click Model (SDCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDCM:\n",
    "    def __init__(self, log_filename):\n",
    "        self.MAX_REL = 2\n",
    "        log_file = open(log_filename)\n",
    "        self.rank_probabilities = self.get_parameters(log_file)\n",
    "        self.attractiveness = lambda x: (2**x-1) / 2**self.MAX_REL\n",
    "        \n",
    "    def _is_querry(self, line):\n",
    "        return line[2].lower() == 'q'\n",
    "\n",
    "    def _get_url_list(self, line):\n",
    "        assert line[2].lower() == 'q'\n",
    "        return line[5:]\n",
    "    \n",
    "    def get_rank(self, querry, click):\n",
    "        if click[3] not in querry[5:]: # weird..\n",
    "            return -1\n",
    "        else:\n",
    "            querry = querry[5:]\n",
    "            return querry.index(click[3])\n",
    "    \n",
    "    def get_parameters(self,training_data):\n",
    "        \"\"\"\n",
    "        (a) A method that learns the parameters of the model given a set of training data.\n",
    "        \"\"\"\n",
    "        last_clicked_rank= -1\n",
    "        last_querry = -1\n",
    "        \n",
    "        last_click_rank_counter = cl.Counter()\n",
    "        click_rank_counter = cl.Counter()\n",
    "        \n",
    "        for line in training_data:\n",
    "            line = line.split()\n",
    "            if self._is_querry(line): # is querry\n",
    "                last_querry = line\n",
    "                if last_clicked_rank != -1:  #the previusly click was the last one\n",
    "                    last_click_rank_counter[last_clicked_rank] += 1\n",
    "                    last_clicked_rank = -1  # we counted it, so we 'remove' it.\n",
    "            else:# is click\n",
    "                last_clicked_rank = self.get_rank(last_querry, line)\n",
    "                click_rank_counter[last_clicked_rank] += 1\n",
    "        # to take into consideration the last click in the log file.\n",
    "        if last_clicked_rank != -1:\n",
    "            last_click_rank_counter[last_clicked_rank] += 1\n",
    "            last_clicked_rank = -1  # we countend, so we 'remove' it.\n",
    "            \n",
    "        return 1 - np.array([last_click_rank_counter[r]/click_rank_counter[r] for r in range(0,10)])\n",
    "    \n",
    "    def click_probabilities(self, urls):\n",
    "        \"\"\"\n",
    "        (b) A method that predicts the link atractiveness given a list of relevance labels.\n",
    "        \"\"\"\n",
    "        return [self.attractiveness(i) for i in urls]\n",
    "    \n",
    "    def clicks(self, atractiveness):\n",
    "        \"\"\"\n",
    "        (c) A method that decides - stochastically - whether a document is clicked based \n",
    "            on their atractiveness and probabilities.\n",
    "        \"\"\"\n",
    "        clicks = np.zeros(len(atractiveness))\n",
    "        for i, a in enumerate(atractiveness):\n",
    "            if np.random.binomial(1, a) == 1:\n",
    "                clicks[i] = 1\n",
    "                if np.random.binomial(1,self.rank_probabilities[i]) == 0: # we should not contiue\n",
    "                    break\n",
    "            else:\n",
    "                clicks[i] = 0\n",
    "        return clicks.astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdcm_model = SDCM(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 :  Simulate Interleaving Experiment ( 10 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(ranking, algorithm, N, model):\n",
    "    E_wins = P_wins = 0\n",
    "    \n",
    "    for k in range(N):\n",
    "        clicks = model.clicks(model.click_probabilities(ranking))\n",
    "        \n",
    "        for i, c in enumerate(clicks):\n",
    "            if c == 1:\n",
    "                algorithm.assign_click(i)\n",
    "\n",
    "        E_score = algorithm.clicks['scores']['E']\n",
    "        P_score = algorithm.clicks['scores']['P']\n",
    "        \n",
    "        if E_score > P_score:\n",
    "            E_wins += 1\n",
    "        elif P_score > E_score:\n",
    "            P_wins += 1\n",
    "            \n",
    "        algorithm.reset_clicks_history()\n",
    "        \n",
    "    return E_wins / (E_wins + P_wins) if E_wins + P_wins != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement done\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "\n",
    "for test_pair in pair_scores:\n",
    "    interleaving_algorithm = Team_Draft_Interleaving()\n",
    "    ranking, e, p = interleaving_algorithm.interleave_pairs(test_pair)\n",
    "    \n",
    "    pair_scores[test_pair]['RCM_E_prop'] = run_simulation(ranking=ranking,\n",
    "                                                          algorithm=interleaving_algorithm,\n",
    "                                                          N=N,\n",
    "                                                          model=rcm_model)\n",
    "    \n",
    "    pair_scores[test_pair]['SDCM_E_prop'] = run_simulation(ranking=ranking,\n",
    "                                                           algorithm=interleaving_algorithm,\n",
    "                                                           N=N,\n",
    "                                                           model=sdcm_model)\n",
    "    \n",
    "print(\"Measurement done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/load scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj(pair_scores, 'pairs_with_scores')\n",
    "# pair_scores = load_obj('pairs_with_scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 :  Results and   Analysis ( 30 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import step_7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
