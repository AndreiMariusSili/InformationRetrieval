{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load statistics from file...Success!\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from Helper import *\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(fpath):\n",
    "    with open(fpath, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def save_pickle(obj, fpath):\n",
    "    with open(fpath, 'wb') as file:\n",
    "        pickle.dump(obj, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_pickle('../pickles/LTR_DF_Training.pkl')\n",
    "validation_data = load_pickle('../pickles/LTR_DF_Validation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[['tfidf', 'LDA', 'LSI', 'dp_mu_500', 'glm', 'doc_len', 'query_len']]\n",
    "y_train = training_data[['relevance_label']]\n",
    "\n",
    "X_validate = validation_data[['tfidf', 'LDA', 'LSI', 'dp_mu_500', 'glm', 'doc_len', 'query_len']]\n",
    "y_validate = validation_data[['relevance_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "normalized_X_validate = (X_validate - X_validate.mean()) / X_validate.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(normalized_X_train.values, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = log_reg.predict(normalized_X_validate.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.DataFrame(columns=df_data.columns)\n",
    "lookup_indices = []\n",
    "\n",
    "with open('../retrievals/tfidf.run', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        query_id, _, ext_doc_id, _, __, ___  = line.split()\n",
    "        \n",
    "        lookup_indices.append('~'.join((query_id, ext_doc_id)))\n",
    "\n",
    "with open('../ap_88_89/qrel_validation', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        query_id, _, ext_doc_id, relevance = line.split()\n",
    "        \n",
    "        idx = '~'.join((query_id, ext_doc_id))\n",
    "        \n",
    "        if idx in lookup_indices:\n",
    "            df_valid.loc[idx, 'query_id'] = query_id\n",
    "            df_valid.loc[idx, 'int_doc_id'] = ext_to_int_dict[ext_doc_id]\n",
    "            df_valid.loc[idx, 'ext_doc_id'] = ext_doc_id\n",
    "            df_valid.loc[idx, 'relevance_label'] = int(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_file(model_name):\n",
    "    print(\"Loading data for model {}\".format(model_name))\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    with open('../retrievals/{}.run'.format(model_name)) as file:\n",
    "        for line in file.readlines():\n",
    "            query_id, _, ext_doc_id, __, score, model = line.split()\n",
    "            idx = '~'.join((query_id, ext_doc_id))\n",
    "\n",
    "            if idx in df_valid.index:\n",
    "                df_valid.loc['~'.join((query_id, ext_doc_id)), model] = float(score)\n",
    "\n",
    "        print(\"Data loaded in {} seconds.\".format(time.time() - retrieval_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for model tfidf\n",
      "Data loaded in 8.319969177246094 seconds.\n",
      "Loading data for model LDA\n",
      "Data loaded in 9.220715045928955 seconds.\n",
      "Loading data for model LSI\n",
      "Data loaded in 8.675105094909668 seconds.\n",
      "Loading data for model dp_mu_500\n",
      "Data loaded in 3.5555970668792725 seconds.\n",
      "Loading data for model glm\n",
      "Data loaded in 0.7149310111999512 seconds.\n"
     ]
    }
   ],
   "source": [
    "models = ['tfidf', 'LDA', 'LSI', 'dp_mu_500', 'glm']\n",
    "for model in models:\n",
    "    load_from_file(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_valid.iterrows():\n",
    "    df_valid.loc[idx, 'doc_len'] = document_lengths[ext_to_int_dict[row['ext_doc_id']]]\n",
    "    df_valid.loc[idx, 'query_len'] = len(tokenized_queries[row['query_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_df = deepcopy(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5598 rows dropped. DataFrame length:450\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for idx, row in df_valid.iterrows():\n",
    "    if row.isnull().any():\n",
    "        i += 1\n",
    "        df_valid.drop(idx, inplace=True, axis=0)\n",
    "\n",
    "print(\"{} rows dropped. DataFrame length:\".format(i), end=\"\")\n",
    "print(len(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6048"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(df_valid, '../pickles/LTR_DF_Validation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_doc_pairs = collections.defaultdict(list)\n",
    "# doc_query_pairs = collections.defaultdict(list)\n",
    "\n",
    "# print(\"Loading relevance indicators.\")\n",
    "# retrieval_start_time = time.time()\n",
    "\n",
    "# with open('../ap_88_89/qrel_test') as file:\n",
    "#     for line in file.readlines():\n",
    "#         query_id, _, ext_doc_id, relevance = line.split()\n",
    "        \n",
    "#         query_doc_pairs[query_id].append(ext_doc_id)\n",
    "#         doc_query_pairs[ext_doc_id].append(query_id)\n",
    "    \n",
    "# print(\"Data loaded in {} seconds.\".format(time.time() - retrieval_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_to_ext_id = collections.defaultdict(int)\n",
    "# ext_to_int_id = collections.defaultdict(int)\n",
    "\n",
    "# for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "#     ext_doc_id, _ = index.document(int_doc_id)\n",
    "#     int_to_ext_id[int_doc_id] = ext_doc_id\n",
    "#     ext_to_int_id[ext_doc_id] = int_doc_id\n",
    "\n",
    "# save_pickle(int_to_ext_id, '../pickles/int_to_ext.pkl')\n",
    "# save_pickle(ext_to_int_id, '../pickles/ext_to_int.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, ranked_data:dict, index:pyndri.Index, models:list, rel_file:str,\n",
    "                 doc_len: dict, int_to_ext_dict: dict, ext_to_int_dict: dict, queries: list):\n",
    "        self.tfidf_data = ranked_data\n",
    "        self.index = index\n",
    "        self.df = None\n",
    "        self.doc_len = doc_len\n",
    "        self.int_to_ext_dict = int_to_ext_dict\n",
    "        self.ext_to_int_dict = ext_to_int_dict\n",
    "        self.queries = queries\n",
    "        \n",
    "        self.index_list = []\n",
    "        self.full_index_list = []\n",
    "        \n",
    "        self.create_df()\n",
    "        self.load_data(models_list=models, relevance_file=rel_file)\n",
    "                \n",
    "    def get_indices_lists(self):\n",
    "        \"\"\"Create the index list based on query ID and external document ID.\"\"\"\n",
    "        self.query_ids = list(self.tfidf_data.keys())\n",
    "        for query_id, int_doc_ids in self.tfidf_data.items():\n",
    "            for int_doc_id in int_doc_ids:\n",
    "                ext_doc_id, _ = index.document(int_doc_id)\n",
    "                self.index_list.append('~'.join((str(query_id), str(ext_doc_id))))\n",
    "                self.full_index_list.append('~'.join((str(query_id), str(int_doc_id), str(ext_doc_id))))\n",
    "                \n",
    "    def create_df(self):\n",
    "        \"\"\"Create initial DataFrame, populating it with useful data.\"\"\"\n",
    "        self.get_indices_lists()\n",
    "        self.df = pd.DataFrame(index=self.index_list)\n",
    "        self.df['idx'] = self.full_index_list\n",
    "        self.df['query_id'] = self.df.idx.apply(lambda x: x.split('~')[0])\n",
    "        self.df['int_doc_id'] = self.df.idx.apply(lambda x: x.split('~')[1])\n",
    "        self.df['ext_doc_id'] = self.df.idx.apply(lambda x: x.split('~')[2])\n",
    "        self.df.drop(['idx'], axis=1, inplace=True)\n",
    "        print(\"DataFrame created.\")\n",
    "        \n",
    "    def load_data_from_file(self, model_name):\n",
    "        \"\"\"Load model scores from file.\n",
    "        \n",
    "        Args:\n",
    "            model_name: name of the model.\n",
    "        \"\"\"\n",
    "        print(\"Loading data for model {}\".format(model_name))\n",
    "        retrieval_start_time = time.time()\n",
    "\n",
    "        with open('../retrievals/{}.run'.format(model_name)) as file:\n",
    "            for line in file.readlines():\n",
    "                if line[:2] not in self.query_ids:\n",
    "                    continue\n",
    "\n",
    "                query_id, _, ext_doc_id, __, score, model = line.split()\n",
    "                idx = '~'.join((query_id, ext_doc_id))\n",
    "        \n",
    "                if idx in self.df.index:\n",
    "                    self.df.loc['~'.join((query_id, ext_doc_id)), model] = float(score)\n",
    "\n",
    "        print(\"Data loaded in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "        \n",
    "    def load_relevance_label(self, file_path):\n",
    "        \"\"\"Load relevance labels from file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: path to the qrel_test file.\n",
    "        \"\"\"\n",
    "        print(\"Loading relevance labels.\")\n",
    "        retrieval_start_time = time.time()\n",
    "\n",
    "        with open(file_path) as file:\n",
    "            for line in file.readlines():\n",
    "                if line[:2] not in self.query_ids:\n",
    "                    continue\n",
    "\n",
    "                query_id, _, ext_doc_id, relevance = line.split()\n",
    "                idx = '~'.join((query_id, ext_doc_id))\n",
    "        \n",
    "                if idx in self.df.index:\n",
    "                    self.df.loc['~'.join((query_id, ext_doc_id)), 'relevance_label'] = int(relevance)\n",
    "\n",
    "        self.df['relevance_label'].fillna(value=0, inplace=True)\n",
    "        print(\"Labels loaded in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "        \n",
    "    def drop_rows_with_null(self):\n",
    "        \"\"\"Drop the rows containing null values.\"\"\"\n",
    "        i = 0\n",
    "        for idx, row in self.df.iterrows():\n",
    "            if row.isnull().any():\n",
    "                i += 1\n",
    "                self.df.drop(idx, inplace=True, axis=0)\n",
    "            \n",
    "        print(\"{} rows dropped. DataFrame length:\".format(i), end=\"\")\n",
    "        print(self.data_length)\n",
    "        \n",
    "    def load_additional_features(self):\n",
    "        for idx, row in self.df.iterrows():\n",
    "            self.df.loc[idx, 'doc_len'] = self.doc_len[self.ext_to_int_dict[row['ext_doc_id']]]\n",
    "            self.df.loc[idx, 'query_len'] = len(self.queries[row['query_id']])\n",
    "        \n",
    "    def load_data(self, models_list, relevance_file):\n",
    "        \"\"\"Wrapper method to load all models scores and relevance labels.\n",
    "        \n",
    "        Args:\n",
    "            models_list: list of model names.\n",
    "            relevance_file: path to the file with relevance labels\n",
    "        \"\"\"\n",
    "        for model in models_list:\n",
    "            self.load_data_from_file(model)\n",
    "        self.drop_rows_with_null()\n",
    "        self.load_additional_features()\n",
    "        self.load_relevance_label(relevance_file)\n",
    "\n",
    "    def data_has_nulls(self):\n",
    "        \"\"\"Check whether df has any null values\"\"\"\n",
    "        return self.df.isnull().any()\n",
    "    \n",
    "    def column_has_nulls(self, col_name):\n",
    "        \"\"\"Check whether a column has any null values.\n",
    "        \n",
    "        Args:\n",
    "            col_name: name of the column.\n",
    "        \"\"\"\n",
    "        return self.df[col_name].isnull().any()\n",
    "    \n",
    "    def count_null_values(self, col_name):\n",
    "        \"\"\"Retrieve the count of null values on a column.\n",
    "        \n",
    "        Args:\n",
    "            col_name: name of the column\n",
    "        \"\"\"\n",
    "        return np.sum(self.df[col_name].isnull())\n",
    "    \n",
    "    def save_dataframe(self, fpath):\n",
    "        \"\"\"Save DataFrame object to file.\n",
    "        \n",
    "        Args:\n",
    "            fpath: file path to save.\n",
    "        \"\"\"\n",
    "        with open(fpath, 'wb') as file:\n",
    "            pickle.dump(self.df, file)\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Retrieve DataFrame object.\"\"\"\n",
    "        if self.df is None:\n",
    "            self.create_df()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    @property\n",
    "    def data_length(self):\n",
    "        \"\"\"Retrieve DataFrame object length.\"\"\"\n",
    "        if self.df is None:\n",
    "            self.create_df()\n",
    "        \n",
    "        return len(self.df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
