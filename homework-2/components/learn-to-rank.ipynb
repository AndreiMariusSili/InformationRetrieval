{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(fpath):\n",
    "    with open(fpath, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "def save_pickle(obj, fpath):\n",
    "    with open(fpath, 'wb') as file:\n",
    "        pickle.dump(obj, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = dict(load_pickle('../pickles/prepro_doc_col_q10_top1000_tfidf.pkl'))\n",
    "index = pyndri.Index('../index/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, tfidf_data:dict, index:pyndri.Index, models:list, rel_file:str):\n",
    "        self.tfidf_data = tfidf_data\n",
    "        self.index = index\n",
    "        self.df = None\n",
    "        \n",
    "        self.index_list = []\n",
    "        self.full_index_list = []\n",
    "        \n",
    "        self.create_df()\n",
    "        self.load_data(models_list=models, relevance_file=rel_file)\n",
    "                \n",
    "    def get_indices_lists(self):\n",
    "        \"\"\"Create the index list based on query ID and external document ID.\"\"\"\n",
    "        self.query_ids = list(self.tfidf_data.keys())\n",
    "        for query_id, int_doc_ids in self.tfidf_data.items():\n",
    "            for int_doc_id in int_doc_ids:\n",
    "                ext_doc_id, _ = index.document(int_doc_id)\n",
    "                self.index_list.append('~'.join((str(query_id), str(ext_doc_id))))\n",
    "                self.full_index_list.append('~'.join((str(query_id), str(int_doc_id), str(ext_doc_id))))\n",
    "                \n",
    "    def create_df(self):\n",
    "        \"\"\"Create initial DataFrame, populating it with useful data.\"\"\"\n",
    "        self.get_indices_lists()\n",
    "        self.df = pd.DataFrame(index=self.index_list)\n",
    "        self.df['idx'] = self.full_index_list\n",
    "        self.df['query_id'] = self.df.idx.apply(lambda x: x.split('~')[0])\n",
    "        self.df['int_doc_id'] = self.df.idx.apply(lambda x: x.split('~')[1])\n",
    "        self.df['ext_doc_id'] = self.df.idx.apply(lambda x: x.split('~')[2])\n",
    "        self.df.drop(['idx'], axis=1, inplace=True)\n",
    "        print(\"DataFrame created.\")\n",
    "        \n",
    "    def load_data_from_file(self, model_name):\n",
    "        \"\"\"Load model scores from file.\n",
    "        \n",
    "        Args:\n",
    "            model_name: name of the model.\n",
    "        \"\"\"\n",
    "        print(\"Loading data for model {}\".format(model_name))\n",
    "        retrieval_start_time = time.time()\n",
    "\n",
    "        with open('../retrievals/{}.run'.format(model_name)) as file:\n",
    "            for line in file.readlines():\n",
    "                if line[:2] not in self.query_ids:\n",
    "                    continue\n",
    "\n",
    "                query_id, _, ext_doc_id, __, score, model = line.split()\n",
    "                idx = '~'.join((query_id, ext_doc_id))\n",
    "        \n",
    "                if idx in self.df.index:\n",
    "                    self.df.loc['~'.join((query_id, ext_doc_id)), model] = float(score)\n",
    "\n",
    "        print(\"Data loaded in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "        \n",
    "    def load_relevance_label(self, file_path):\n",
    "        \"\"\"Load relevance labels from file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: path to the qrel_test file.\n",
    "        \"\"\"\n",
    "        print(\"Loading relevance labels.\")\n",
    "        retrieval_start_time = time.time()\n",
    "\n",
    "        with open(file_path) as file:\n",
    "            for line in file.readlines():\n",
    "                if line[:2] not in self.query_ids:\n",
    "                    continue\n",
    "\n",
    "                query_id, _, ext_doc_id, relevance = line.split()\n",
    "                idx = '~'.join((query_id, ext_doc_id))\n",
    "        \n",
    "                if idx in self.df.index:\n",
    "                    self.df.loc['~'.join((query_id, ext_doc_id)), 'relevance_label'] = int(relevance)\n",
    "\n",
    "        self.df['relevance_label'].fillna(value=0, inplace=True)\n",
    "        print(\"Labels loaded in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "        \n",
    "    def drop_rows_with_null(self):\n",
    "        \"\"\"Drop the rows containing null values.\"\"\"\n",
    "        i = 0\n",
    "        for idx, row in self.df.iterrows():\n",
    "            if row.isnull().any():\n",
    "                i += 1\n",
    "                self.df.drop(idx, inplace=True, axis=0)\n",
    "            \n",
    "        print(\"{} rows dropped. DataFrame length:\".format(i), end=\"\")\n",
    "        print(self.data_length)\n",
    "        \n",
    "    def load_data(self, models_list, relevance_file):\n",
    "        \"\"\"Wrapper method to load all models scores and relevance labels.\n",
    "        \n",
    "        Args:\n",
    "            models_list: list of model names.\n",
    "            relevance_file: path to the file with relevance labels\n",
    "        \"\"\"\n",
    "        for model in models_list:\n",
    "            self.load_data_from_file(model)\n",
    "        self.drop_rows_with_null()\n",
    "        self.load_relevance_label(relevance_file)\n",
    "\n",
    "    def data_has_nulls(self):\n",
    "        \"\"\"Check whether df has any null values\"\"\"\n",
    "        return self.df.isnull().any()\n",
    "    \n",
    "    def column_has_nulls(self, col_name):\n",
    "        \"\"\"Check whether a column has any null values.\n",
    "        \n",
    "        Args:\n",
    "            col_name: name of the column.\n",
    "        \"\"\"\n",
    "        return self.df[col_name].isnull().any()\n",
    "    \n",
    "    def count_null_values(self, col_name):\n",
    "        \"\"\"Retrieve the count of null values on a column.\n",
    "        \n",
    "        Args:\n",
    "            col_name: name of the column\n",
    "        \"\"\"\n",
    "        return np.sum(self.df[col_name].isnull())\n",
    "    \n",
    "    def save_dataframe(self, fpath):\n",
    "        \"\"\"Save DataFrame object to file.\n",
    "        \n",
    "        Args:\n",
    "            fpath: file path to save.\n",
    "        \"\"\"\n",
    "        with open(fpath, 'wb') as file:\n",
    "            pickle.dump(self.df, file)\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Retrieve DataFrame object.\"\"\"\n",
    "        if self.df is None:\n",
    "            self.create_df()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    @property\n",
    "    def data_length(self):\n",
    "        \"\"\"Retrieve DataFrame object length.\"\"\"\n",
    "        if self.df is None:\n",
    "            self.create_df()\n",
    "        \n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created.\n",
      "Loading data for model tfidf\n",
      "Data loaded in 5.246838092803955 seconds.\n",
      "Loading data for model LDA\n",
      "Data loaded in 5.35348916053772 seconds.\n",
      "Loading data for model LSI\n",
      "Data loaded in 6.03591513633728 seconds.\n",
      "Loading data for model jm_lambda_0.1\n",
      "Data loaded in 2.3770501613616943 seconds.\n",
      "Loading data for model dp_mu_500\n",
      "Data loaded in 3.2428719997406006 seconds.\n",
      "Loading data for model ad_delta_0.9\n",
      "Data loaded in 2.764694929122925 seconds.\n",
      "Loading data for model glm\n",
      "Data loaded in 5.0609190464019775 seconds.\n",
      "5031 rows dropped. DataFrame length:4126\n",
      "Loading relevance labels.\n",
      "Labels loaded in 0.2678070068359375 seconds.\n"
     ]
    }
   ],
   "source": [
    "models = ['tfidf', 'LDA', 'LSI', 'jm_lambda_0.1', 'dp_mu_500', 'ad_delta_0.9', 'glm']\n",
    "rel_file = '../ap_88_89/qrel_test'\n",
    "data_loader = DataLoader(tfidf_data, index, models, rel_file)\n",
    "data_loader.save_dataframe('../pickles/LTR_DataFrame.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = load_pickle('../pickles/LTR_DataFrame.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_data[['tfidf', 'LDA', 'LSI', 'jm_lambda_0.1', 'dp_mu_500', 'ad_delta_0.9', 'glm']]\n",
    "y_train = df_data[['relevance_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X = (X_train - X_train.mean()) / X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(normalized_X.values, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_doc_pairs = collections.defaultdict(list)\n",
    "# doc_query_pairs = collections.defaultdict(list)\n",
    "\n",
    "# print(\"Loading relevance indicators.\")\n",
    "# retrieval_start_time = time.time()\n",
    "\n",
    "# with open('../ap_88_89/qrel_test') as file:\n",
    "#     for line in file.readlines():\n",
    "#         query_id, _, ext_doc_id, relevance = line.split()\n",
    "        \n",
    "#         query_doc_pairs[query_id].append(ext_doc_id)\n",
    "#         doc_query_pairs[ext_doc_id].append(query_id)\n",
    "    \n",
    "# print(\"Data loaded in {} seconds.\".format(time.time() - retrieval_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_to_ext_id = collections.defaultdict(int)\n",
    "# ext_to_int_id = collections.defaultdict(int)\n",
    "\n",
    "# for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "#     ext_doc_id, _ = index.document(int_doc_id)\n",
    "#     int_to_ext_id[int_doc_id] = ext_doc_id\n",
    "#     ext_to_int_id[ext_doc_id] = int_doc_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
