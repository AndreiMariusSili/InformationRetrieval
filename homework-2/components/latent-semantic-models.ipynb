{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import io\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pyndri.Index('../index/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "\n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "with open('../ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load statistics from file...Success!\n"
     ]
    }
   ],
   "source": [
    "with open('../ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('../index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "# inverted index creation.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "try:\n",
    "    print('Trying to load statistics from file...', end='')\n",
    "    with open('../pickles/inverted_index.pkl', 'rb') as file:\n",
    "        inverted_index = pickle.load(file)\n",
    "    with open('../pickles/collection_frequencies.pkl', 'rb') as file:\n",
    "        collection_frequencies = pickle.load(file)\n",
    "    with open('../pickles/document_lengths.pkl', 'rb') as file:\n",
    "        document_lengths = pickle.load(file)\n",
    "    with open('../pickles/unique_terms_per_document.pkl', 'rb') as file:\n",
    "        unique_terms_per_document = pickle.load(file)\n",
    "    with open('../pickles/document_term_frequency.pkl', 'rb') as file:\n",
    "        document_term_frequency = pickle.load(file)\n",
    "    with open('../pickles/avg_doc_length.pkl', 'rb') as file:\n",
    "        avg_doc_length = pickle.load(file)\n",
    "    print('Success!')\n",
    "except FileNotFoundError:\n",
    "    print('Error!')\n",
    "    print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "    inverted_index = collections.defaultdict(dict)\n",
    "    collection_frequencies = collections.defaultdict(int)\n",
    "\n",
    "    total_terms = 0\n",
    "\n",
    "    for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "        ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "        document_bow = collections.Counter(\n",
    "            token_id for token_id in doc_token_ids\n",
    "            if token_id > 0)\n",
    "        document_length = sum(document_bow.values())\n",
    "\n",
    "        document_lengths[int_doc_id] = document_length\n",
    "        total_terms += document_length\n",
    "\n",
    "        unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "        for query_term_id in query_term_ids:\n",
    "            assert query_term_id is not None\n",
    "\n",
    "            document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "            if document_term_frequency == 0:\n",
    "                continue\n",
    "\n",
    "            collection_frequencies[query_term_id] += document_term_frequency\n",
    "            inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "\n",
    "    avg_doc_length = total_terms / num_documents\n",
    "\n",
    "    print('Inverted index creation took', time.time() - start_time, 'seconds.')\n",
    "\n",
    "    print('Saving statistics for future use...', end='')\n",
    "    with open('../pickles/inverted_index.pkl', 'wb') as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "    with open('../pickles/collection_frequencies.pkl', 'wb') as file:\n",
    "        pickle.dump(collection_frequencies, file)\n",
    "    with open('../pickles/document_lengths.pkl', 'wb') as file:\n",
    "        pickle.dump(document_lengths, file)\n",
    "    with open('../pickles/unique_terms_per_document.pkl', 'wb') as file:\n",
    "        pickle.dump(unique_terms_per_document, file)\n",
    "    with open('../pickles/document_term_frequency.pkl', 'wb') as file:\n",
    "        pickle.dump(document_term_frequency, file)\n",
    "    with open('../pickles/avg_doc_length.pkl', 'wb') as file:\n",
    "        pickle.dump(avg_doc_length, file)\n",
    "    print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/preprocessed_tfidf_collection.pkl', 'rb') as file:\n",
    "    tfidf_data = dict(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentences2Vec(pyndri.compat.IndriSentences):\n",
    "    \"\"\"IndriSentences own class implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, index, dictionary, max_documents=None):\n",
    "        super().__init__(index, dictionary, max_documents)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for int_doc_id in range(self.index.document_base(),\n",
    "                                self._maximum_document()):\n",
    "            ext_doc_id, doc = self.index.document(int_doc_id)\n",
    "            tokens_bow = self.dictionary.doc2bow(doc)\n",
    "\n",
    "            yield tuple(\n",
    "                (token_id, weight)\n",
    "                for (token_id, weight) in tokens_bow\n",
    "                if token_id in self.dictionary and token_id > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSemanticIndexing():\n",
    "    \"\"\"Latent Semantic Indexing method implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, index: pyndri.Index, dictionary: dict, num_topics=200, load_model=False, fname=\"\"):\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "        if load_model:\n",
    "            if fname == \"\":\n",
    "                raise ValueError('File path not provided.')\n",
    "            self.load(fname)\n",
    "        else:\n",
    "            self.train(num_topics)\n",
    "            \n",
    "        self.doc_representations_dict = collections.defaultdict(list)\n",
    "        self.load_documents_representation()\n",
    "            \n",
    "    @property\n",
    "    def model_name(self):\n",
    "        \"\"\"Model name\"\"\"\n",
    "        return \"LSI\"\n",
    "        \n",
    "    def load(self, fpath: str):\n",
    "        \"\"\"Load model from file.\n",
    "        \n",
    "        Args:\n",
    "            fpath: file path to load model.\n",
    "        \"\"\"\n",
    "        self.model = gensim.models.lsimodel.LsiModel.load(fpath)\n",
    "        print(\"Model loaded.\")\n",
    "        \n",
    "    def save(self, fpath: str):\n",
    "        \"\"\"Save current model to file for further use.\n",
    "        \n",
    "        Args:\n",
    "            fname: file path to save model.\n",
    "        \"\"\"\n",
    "        self.model.save(fpath)\n",
    "        print(\"Model saved.\")        \n",
    "        \n",
    "    def train(self, num_topics=200):\n",
    "        \"\"\"Train LSI model given the index and dictionary.\"\"\"\n",
    "        print(\"Training started...\")\n",
    "        retrieval_start_time = time.time()\n",
    "        \n",
    "        corpus = Sentences2Vec(self.index, self.dictionary)\n",
    "        self.model = gensim.models.lsimodel.LsiModel(corpus=corpus,\n",
    "                                                     id2word=self.dictionary.id2token,\n",
    "                                                     num_topics=num_topics)\n",
    "        print(\"Model trained in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "        \n",
    "    def load_documents_representation(self):\n",
    "        \"\"\"Get and store document representations for future use.\"\"\"\n",
    "        try:\n",
    "            print(\"Loading document representations from file...\", end='')\n",
    "            with open('../pickles/LSI_DocRepresentations.pkl', 'rb') as file:\n",
    "                self.doc_representations_dict = pickle.load(file)\n",
    "            print(\"Success!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error!\")\n",
    "            print(\"Computing and loading documents' representations...\")\n",
    "    \n",
    "            retrieval_start_time = time.time()\n",
    "            for int_doc_id in range(self.index.document_base(), self.index.maximum_document()):\n",
    "                ext_doc_id, doc = self.index.document(int_doc_id)\n",
    "                self.doc_representations_dict[int_doc_id] = self.get_representation(doc)\n",
    "            \n",
    "            print(\"Documents successfully loaded in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "            \n",
    "            with open('../pickles/LSI_DocRepresentations.pkl', 'wb') as file:\n",
    "                pickle.dump(self.doc_representations_dict, file)\n",
    "        \n",
    "    def get_representation(self, tokens_list):\n",
    "        \"\"\"Build representation given list of token ids.\n",
    "        \n",
    "        Args:\n",
    "            tokens_list: list of token ids.\n",
    "        Return:\n",
    "            List of the LSI representation.\n",
    "        \"\"\"\n",
    "        tokens_bow = self.dictionary.doc2bow(tokens_list)\n",
    "        doc_representation = [(token_id, weight)\n",
    "                              for (token_id, weight) in tokens_bow\n",
    "                              if token_id in self.dictionary and token_id > 0]\n",
    "        lsi_repr = [x[1] for x in self.model[doc_representation]]\n",
    "        return lsi_repr\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Compute cosine similarity of 2 vectors.\n",
    "        \n",
    "        Args:\n",
    "            vec1: 1st vector.\n",
    "            vec2: 2nd vector.\n",
    "        Return:\n",
    "            Cosine similarity.\n",
    "        \"\"\"\n",
    "        dot_prod = np.dot(vec1, vec2)\n",
    "        norm_1 = np.linalg.norm(vec1)\n",
    "        norm_2 = np.linalg.norm(vec2)\n",
    "        return dot_prod / (norm_1 * norm_2)\n",
    "    \n",
    "    def run_retrieval(self, tfidf_data):\n",
    "        \"\"\"\n",
    "        Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "\n",
    "        Args:\n",
    "            tfidf_data: top-1000 query-document rankings from TF-IDF.\n",
    "        \"\"\"\n",
    "        run_out_path = '{}.run'.format(self.model_name)\n",
    "\n",
    "        if os.path.exists(run_out_path):\n",
    "            print('RUN file already existing')\n",
    "            return\n",
    "        \n",
    "        data = collections.defaultdict(list)\n",
    "        \n",
    "        print('Retrieving using {}'.format(self.model_name))\n",
    "        retrieval_start_time = time.time()\n",
    "        \n",
    "        for query_id, doc_list in tfidf_data.items():\n",
    "            query_representation = self.get_representation(tokenized_queries[query_id])\n",
    "            \n",
    "            for int_doc_id in doc_list:\n",
    "                ext_doc_id, doc = index.document(int_doc_id)\n",
    "                doc_representation = self.doc_representations_dict[int_doc_id]\n",
    "                \n",
    "                cos_similarity = self.cosine_similarity(query_representation, doc_representation)            \n",
    "                data[query_id].append((cos_similarity, ext_doc_id))\n",
    "            \n",
    "            data[query_id] = sorted(data[query_id], reverse=True)\n",
    "        \n",
    "        with open(run_out_path, 'w') as f_out:\n",
    "            write_run(\n",
    "                model_name=self.model_name,\n",
    "                data=data,\n",
    "                out_f=f_out,\n",
    "                max_objects_per_query=1000)\n",
    "            \n",
    "        print('Retrieval run took {} seconds.'.format(time.time() - retrieval_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Loading document representations from file...Success!\n"
     ]
    }
   ],
   "source": [
    "# lsi_model = LatentSemanticIndexing(index, dictionary, num_topics=len(queries))\n",
    "lsi_model = LatentSemanticIndexing(index, dictionary, load_model=True, fname='../models/new_lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using LSI\n",
      "Retrieval run took 102.59836983680725 seconds.\n"
     ]
    }
   ],
   "source": [
    "lsi_model.run_retrieval(tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi_model.save('../models/new_lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDirichletAllocation():\n",
    "    \"\"\"Latent Dirichlet Allocation method implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, index: pyndri.Index, dictionary: dict, num_topics=200, load_model=False, fname=\"\"):\n",
    "        self.index = index\n",
    "        self.dictionary = dictionary\n",
    "        self.num_topics = num_topics\n",
    "        \n",
    "        if load_model:\n",
    "            if fname == \"\":\n",
    "                raise ValueError('File path not provided.')\n",
    "            self.load(fname)\n",
    "        else:\n",
    "            self.train()\n",
    "            \n",
    "        self.doc_representations_dict = collections.defaultdict(list)\n",
    "        self.load_documents_representation()\n",
    "            \n",
    "    @property\n",
    "    def model_name(self):\n",
    "        \"\"\"Model name\"\"\"\n",
    "        return \"LDA\"\n",
    "        \n",
    "    def load(self, fpath: str):\n",
    "        \"\"\"Load model from file.\n",
    "        \n",
    "        Args:\n",
    "            fpath: file path to load model.\n",
    "        \"\"\"\n",
    "        self.model = gensim.models.ldamodel.LdaModel.load(fpath)\n",
    "        print(\"Model loaded.\")\n",
    "        \n",
    "    def save(self, fpath: str):\n",
    "        \"\"\"Save current model to file for further use.\n",
    "        \n",
    "        Args:\n",
    "            fname: file path to save model.\n",
    "        \"\"\"\n",
    "        self.model.save(fpath)\n",
    "        print(\"Model saved.\")        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train LDA model given the index and dictionary.\"\"\"\n",
    "        print(\"Training started...\")\n",
    "        retrieval_start_time = time.time()\n",
    "        \n",
    "        corpus = Sentences2Vec(self.index, self.dictionary)\n",
    "        self.model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                     id2word=self.dictionary.id2token,\n",
    "                                                     num_topics=self.num_topics,\n",
    "                                                     update_every=1,\n",
    "                                                     chunksize=10000,\n",
    "                                                     passes=1)\n",
    "        print(\"Model trained in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "        \n",
    "    def load_documents_representation(self):\n",
    "        \"\"\"Get and store document representations for future use.\"\"\"\n",
    "        try:\n",
    "            print(\"Loading document representations from file...\", end='')\n",
    "            with open('../pickles/LDA_DocRepresentations.pkl', 'rb') as file:\n",
    "                self.doc_representations_dict = pickle.load(file)\n",
    "            print(\"Success!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error!\")\n",
    "            print(\"Computing and loading documents' representations...\")\n",
    "    \n",
    "            retrieval_start_time = time.time()\n",
    "            for int_doc_id in range(self.index.document_base(), self.index.maximum_document()):\n",
    "                ext_doc_id, doc = self.index.document(int_doc_id)\n",
    "                self.doc_representations_dict[int_doc_id] = self.get_representation(doc)\n",
    "            \n",
    "            print(\"Documents successfully loaded in {} seconds.\".format(time.time() - retrieval_start_time))\n",
    "            \n",
    "            with open('../pickles/LDA_DocRepresentations.pkl', 'wb') as file:\n",
    "                pickle.dump(self.doc_representations_dict, file)\n",
    "        \n",
    "    def get_representation(self, tokens_list):\n",
    "        \"\"\"Build representation given list of token ids.\n",
    "        \n",
    "        Args:\n",
    "            tokens_list: list of token ids.\n",
    "        Return:\n",
    "            List of the LDA representation.\n",
    "        \"\"\"\n",
    "        tokens_bow = self.dictionary.doc2bow(tokens_list)\n",
    "        doc_representation = [(token_id, weight)\n",
    "                              for (token_id, weight) in tokens_bow\n",
    "                              if token_id in self.dictionary and token_id > 0]\n",
    "        lda_repr = np.zeros(self.num_topics)\n",
    "        for index, value in self.model[doc_representation]:\n",
    "            lda_repr[index] = value\n",
    "        return lda_repr\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Compute cosine similarity of 2 vectors.\n",
    "        \n",
    "        Args:\n",
    "            vec1: 1st vector.\n",
    "            vec2: 2nd vector.\n",
    "        Return:\n",
    "            Cosine similarity.\n",
    "        \"\"\"\n",
    "        dot_prod = np.dot(vec1, vec2)\n",
    "        norm_1 = np.linalg.norm(vec1)\n",
    "        norm_2 = np.linalg.norm(vec2)\n",
    "        return dot_prod / (norm_1 * norm_2)\n",
    "    \n",
    "    def run_retrieval(self, tfidf_data):\n",
    "        \"\"\"\n",
    "        Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "\n",
    "        Args:\n",
    "            tfidf_data: top-1000 query-document rankings from TF-IDF.\n",
    "        \"\"\"\n",
    "        run_out_path = '{}.run'.format(self.model_name)\n",
    "\n",
    "        if os.path.exists(run_out_path):\n",
    "            print('RUN file already existing')\n",
    "            return\n",
    "        \n",
    "        data = collections.defaultdict(list)\n",
    "        \n",
    "        print('Retrieving using {}'.format(self.model_name))\n",
    "        retrieval_start_time = time.time()\n",
    "        \n",
    "        for query_id, doc_list in tfidf_data.items():\n",
    "            query_representation = self.get_representation(tokenized_queries[query_id])\n",
    "            \n",
    "            for int_doc_id in doc_list:\n",
    "                ext_doc_id, doc = index.document(int_doc_id)\n",
    "                doc_representation = self.doc_representations_dict[int_doc_id]\n",
    "                \n",
    "                cos_similarity = self.cosine_similarity(query_representation, doc_representation)            \n",
    "                data[query_id].append((cos_similarity, ext_doc_id))\n",
    "            \n",
    "            data[query_id] = sorted(data[query_id], reverse=True)\n",
    "        \n",
    "        with open(run_out_path, 'w') as f_out:\n",
    "            write_run(\n",
    "                model_name=self.model_name,\n",
    "                data=data,\n",
    "                out_f=f_out,\n",
    "                max_objects_per_query=1000)\n",
    "            \n",
    "        print('Retrieval run took {} seconds.'.format(time.time() - retrieval_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Loading document representations from file...Success!\n"
     ]
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(index, dictionary, load_model=True, fname='../models/lda_model')\n",
    "# lda_model.save('../models/lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using LDA\n",
      "Retrieval run took 41.169721841812134 seconds.\n"
     ]
    }
   ],
   "source": [
    "lda_model.run_retrieval(tfidf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi_scoring = collections.defaultdict(list)\n",
    "# mapped_data = collections.defaultdict(list)\n",
    "\n",
    "# for query_id, doc_list in data.items():\n",
    "#     query_representation = lsi_model.get_representation(tokenized_queries[query_id])\n",
    "    \n",
    "#     for int_doc_id in doc_list:\n",
    "#         ext_doc_id, doc = index.document(int_doc_id)        \n",
    "#         doc_representation = lsi_model.get_representation(doc)\n",
    "#         cos_similarity = cosine_similarity(query_representation, doc_representation)\n",
    "        \n",
    "#         lsi_scoring[query_id].append((cos_similarity, int_doc_id))\n",
    "        \n",
    "#     lsi_scoring[query_id] = sorted(lsi_scoring[query_id], reverse=True)\n",
    "#     mapped_data[query_id] = list(map(lambda item: item[1], lsi_scoring[query_id]))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csc_matrix = gensim.matutils.corpus2csc(sentences)\n",
    "# corpus = gensim.matutils.Sparse2Corpus(csc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = gensim.models.tfidfmodel.TfidfModel(sentences)\n",
    "# corpus_tfidf = tfidf[sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
